<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>0aef7486acb641959a62120a96a9424d</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-collapsed="true" id="4FUbZUM0Aq5S"
data-outputId="d867ecb6-6826-44ee-e005-a0d41ce1bb10">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install datasets transformers</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)
Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)
Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)
Requirement already satisfied: pyarrow&gt;=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)
Requirement already satisfied: dill&lt;0.3.8,&gt;=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)
Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)
Requirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)
Requirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)
Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)
Requirement already satisfied: fsspec&gt;=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]&gt;=2021.11.1-&gt;datasets) (2025.3.2)
Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)
Requirement already satisfied: huggingface-hub&lt;1.0.0,&gt;=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)
Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)
Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)
Requirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)
Requirement already satisfied: safetensors&gt;=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)
Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (2.6.1)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.4.0)
Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (25.3.0)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.7.0)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (6.6.3)
Requirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (0.3.2)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.20.1)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub&lt;1.0.0,&gt;=0.14.0-&gt;datasets) (4.14.1)
Requirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub&lt;1.0.0,&gt;=0.14.0-&gt;datasets) (1.1.5)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.4.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.10)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2.4.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2025.7.9)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2.9.0.post0)
Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.2)
Requirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.2)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.17.0)
</code></pre>
</div>
</div>
<div class="cell code" id="0hWcYCULBuBm">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Import Libraries</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> Tokenizer</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.models <span class="im">import</span> WordPiece</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> normalizers</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.normalizers <span class="im">import</span> NFD, Lowercase, StripAccents</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.pre_tokenizers <span class="im">import</span> Whitespace</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.trainers <span class="im">import</span> WordPieceTrainer</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> decoders</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> RobertaTokenizer</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForMaskedLM, Trainer, TrainingArguments, pipeline</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset</span></code></pre></div>
</div>
<div class="cell code" id="9OeUZJBnHsD2">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data paths</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>train_data_path <span class="op">=</span> <span class="st">&#39;/content/drive/MyDrive/PLM/ SwissprotDatasets/BalancedSwissprot/train.csv&#39;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>test_data_path <span class="op">=</span> <span class="st">&#39;/content/drive/MyDrive/PLM/ SwissprotDatasets/BalancedSwissprot/valid.csv&#39;</span></span></code></pre></div>
</div>
<div class="cell code" id="ZQP2XqYDZmv9">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load your train data</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(train_data_path)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(test_data_path)</span></code></pre></div>
</div>
<div class="cell code" id="K04mKk40ZsyU">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> train_df[train_df[<span class="st">&#39;Sequence&#39;</span>].<span class="bu">str</span>.<span class="bu">len</span>() <span class="op">&gt;</span> <span class="dv">20</span>]  <span class="co"># remove short sequences</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> test_df[test_df[<span class="st">&#39;Sequence&#39;</span>].<span class="bu">str</span>.<span class="bu">len</span>() <span class="op">&gt;</span> <span class="dv">20</span>]  <span class="co"># remove short sequences</span></span></code></pre></div>
</div>
<div class="cell code" id="g8_gF_yEekwb">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> train_df.iloc[<span class="dv">0</span>:<span class="dv">5000</span>,:]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> test_df.iloc[<span class="dv">0</span>:<span class="dv">2000</span>,:]</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="3IYURsV1ec1r" data-outputId="e870c8c8-987b-4c54-d728-3827f77568b1">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>train_df.shape, test_df.shape</span></code></pre></div>
<div class="output execute_result" data-execution_count="7">
<pre><code>((5000, 8), (2000, 8))</code></pre>
</div>
</div>
<div class="cell code" id="QxcZGm77ZxE7">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === Format sequences with spaces between amino acids ===</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_sequence(seq):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">&#39; &#39;</span>.join(<span class="bu">list</span>(seq.strip()))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>train_df[<span class="st">&#39;spaced_sequence&#39;</span>] <span class="op">=</span> train_df[<span class="st">&#39;Sequence&#39;</span>].<span class="bu">apply</span>(format_sequence)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>test_df[<span class="st">&#39;spaced_sequence&#39;</span>] <span class="op">=</span> test_df[<span class="st">&#39;Sequence&#39;</span>].<span class="bu">apply</span>(format_sequence)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:458}"
id="lZTrFu1YxlvL" data-outputId="204fdaa8-8ad9-4ac7-d225-eace2482f996">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>train_df[<span class="st">&#39;spaced_sequence&#39;</span>]</span></code></pre></div>
<div class="output execute_result" data-execution_count="9">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>spaced_sequence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>A T G T C G G A C G G C G C G G T G G T A C G ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A T G A G A G C A G T T A G A T T A G T A G A ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>A T G A C C G C A A T G A T G A A A G C C G C ...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>G T G A A A G C A G C A G T A G T T A A C G A ...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>A T G A A A A C C A C C G C G G C G G T A C T ...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>4995</th>
      <td>A T G C A A T T A G A T G A A C A A C G T C T ...</td>
    </tr>
    <tr>
      <th>4996</th>
      <td>A T G C A A G T A G A T G A A C A A C G T C T ...</td>
    </tr>
    <tr>
      <th>4997</th>
      <td>A T G C A A G T A G A T G A A C A A C G T C T ...</td>
    </tr>
    <tr>
      <th>4998</th>
      <td>A T G C A A G T A G A T G A A C A A C G T C T ...</td>
    </tr>
    <tr>
      <th>4999</th>
      <td>A T G C C C C C C G A A C C C C T G T C C C T ...</td>
    </tr>
  </tbody>
</table>
<p>5000 rows × 1 columns</p>
</div><br><label><b>dtype:</b> object</label>
</div>
</div>
<div class="cell code" id="A1D_ao1AZ1OH">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === Convert to HuggingFace Dataset ===</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>dataset_train <span class="op">=</span> Dataset.from_pandas(train_df[[<span class="st">&#39;spaced_sequence&#39;</span>]])</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>dataset_test <span class="op">=</span> Dataset.from_pandas(test_df[[<span class="st">&#39;spaced_sequence&#39;</span>]])</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:249,&quot;referenced_widgets&quot;:[&quot;27c23e34d71e4d50845f2143d9ef94b7&quot;,&quot;268595d0a4d04478a2fcd4b6a785e068&quot;,&quot;f99c5a82bc4241da937c81a6f10b23bf&quot;,&quot;35c99b90d82b46d7ad0e4801704c2e47&quot;,&quot;f4147cb48e034d84a087193731952fac&quot;,&quot;ec29312e48c54c5a8627ab6008f5490d&quot;,&quot;8dba8dede16944e0a86eb0c3087005bf&quot;,&quot;2696fb70a6084757a58542886000f261&quot;,&quot;df1ca0dfe596441ca3b6036408fc623b&quot;,&quot;ff84d910858d44c8a8a83cc4a271c350&quot;,&quot;2f279ac158604c04ad71ee7557597cb4&quot;,&quot;13b3b58579ce4c48a6b30693e38a93e9&quot;,&quot;e40d79120d6e411eb46629aaf4e7c1ba&quot;,&quot;3e3a5793dc8543ada8c4961a93ef7737&quot;,&quot;9fb32e0cf4b9443c9400999d948253d7&quot;,&quot;c2fcad232b1a466db89d3347aea948c4&quot;,&quot;1fa5552ad95e49f0bcb7daf06786fd23&quot;,&quot;00fdea2db24d433b9f3ed180c1078e82&quot;,&quot;70165372d7b246ac84eccc474b17fad5&quot;,&quot;dc80f6955769495d9a6297f60aeb9824&quot;,&quot;a35eef8d667349ba904eb068e7131179&quot;,&quot;4709911dabcf46ebbc19eee761689398&quot;,&quot;08ca099b3cf244b6a3d04031cbfc22d8&quot;,&quot;4e16e7f6e0a74dd3946f27e6ee9df9be&quot;,&quot;eb0dbd8f203548d4a81a26dd6cd15ce4&quot;,&quot;2ca20dbe12894a6b93fc2fc2d184f52b&quot;,&quot;a66959c96ae843febd8f9189efad9628&quot;,&quot;b9b60710d68a496ab69eba3aa2363488&quot;,&quot;5e82834c423b4d1581f605f962cc1449&quot;,&quot;e8df267c683644449ead78483caf1ffc&quot;,&quot;9aaf3e0aca524d6b9c26e7b4330ad414&quot;,&quot;f2beef7e33d44cfe80d6f78482edaffd&quot;,&quot;4679b87c98d04a368ee297513b259ec4&quot;,&quot;b48c6ab6abcb4e64be906d121a32852d&quot;,&quot;e5935c681a0a497cae4fc2a75fd921c3&quot;,&quot;865d5682fcc644008319470df3cc25bc&quot;,&quot;1b059e9d040f4eeea3b82d322e149974&quot;,&quot;abff81443aff400fa4db0ec69b57d092&quot;,&quot;8281a9cec7ec4cad8de3ac251b616a5c&quot;,&quot;bab118410bff45a5abbb60305c17cad6&quot;,&quot;35edc92bc32646889018f188b062bf53&quot;,&quot;0d6341100929460cb0f4debcc4093ef0&quot;,&quot;adf6dcfdd6b84bcc8449066c7ebcb2ef&quot;,&quot;6e9f2793fc3447f7972bb89eb1e407b5&quot;]}"
id="ORsL7Jq4Z76Z" data-outputId="2238ac88-9d7c-41ec-c053-2e7de4ecbd70">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">&quot;Rostlab/prot_bert&quot;</span>, do_lower_case<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
</code></pre>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb15"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;27c23e34d71e4d50845f2143d9ef94b7&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb16"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;13b3b58579ce4c48a6b30693e38a93e9&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb17"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;08ca099b3cf244b6a3d04031cbfc22d8&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb18"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;b48c6ab6abcb4e64be906d121a32852d&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:81,&quot;referenced_widgets&quot;:[&quot;adda20b09ba247928e750c6f377f0d25&quot;,&quot;3acedeb2069847eeb6a87ad9697b7096&quot;,&quot;a78b70ce7f404cd49f4ee9a2c4a51b31&quot;,&quot;46adf5e4729144b29f1d868f72a7fdb2&quot;,&quot;8f81e5e4e5bb4e2ab82eb435a6399c79&quot;,&quot;91c2c16a559645e2b51620b249fa25f4&quot;,&quot;214478161d854118b415163295fe6b76&quot;,&quot;f26860238446486faa32f8334ee89958&quot;,&quot;867abd69013f498495b71d836bf0b5c1&quot;,&quot;36f4c36df62e402ea7e58bbec844b72e&quot;,&quot;a233d44d25fb42998912d95404f8ed79&quot;,&quot;acd5a85043d54dd8a7856259c8412334&quot;,&quot;57cf4fc388af49ba9f687df14ba8b472&quot;,&quot;d753bb121e0f4a70b5db5aa1f5b8754d&quot;,&quot;79af865bc8dc4e388db61196083ac5b0&quot;,&quot;8c8ef7dc11234a69a54198f67fc9f5a6&quot;,&quot;80df20a0d3a54ced8f2b84241be9c953&quot;,&quot;e8e4f4bfd78e41af9c9b09368601d1e5&quot;,&quot;d1abe87cb0fc4d249b09bade5fa36d42&quot;,&quot;0479b26244ca4ba1af1cdbd1bbaf84a1&quot;,&quot;70180f5607944c8b801178604b8bd509&quot;,&quot;2d7e47adf5c943a0b4e2b656b3477ad2&quot;]}"
id="WzgYvV7iZ-ux" data-outputId="36ad157b-1e4f-4b8d-bd47-02b9c691bc48">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === Tokenize sequences ===</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_fn(examples):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        examples[<span class="st">&#39;spaced_sequence&#39;</span>],</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">&#39;max_length&#39;</span>,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span><span class="dv">512</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>tokenized_dataset_train <span class="op">=</span> dataset_train.<span class="bu">map</span>(tokenize_fn, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>tokenized_dataset_test <span class="op">=</span> dataset_test.<span class="bu">map</span>(tokenize_fn, batched<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb20"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;adda20b09ba247928e750c6f377f0d25&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb21"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;acd5a85043d54dd8a7856259c8412334&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div class="cell code" id="o_sHUxk3aCkR">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === Data Collator for 15% Random Masking ===</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling(</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    mlm<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    mlm_probability<span class="op">=</span><span class="fl">0.15</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="n1JB86kIyUnF" data-outputId="6cd26259-2083-4373-d992-b25e03fbd7a0">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>data_collator</span></code></pre></div>
<div class="output execute_result" data-execution_count="25">
<pre><code>DataCollatorForLanguageModeling(tokenizer=BertTokenizerFast(name_or_path=&#39;Rostlab/prot_bert&#39;, vocab_size=30, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side=&#39;right&#39;, truncation_side=&#39;right&#39;, special_tokens={&#39;unk_token&#39;: &#39;[UNK]&#39;, &#39;sep_token&#39;: &#39;[SEP]&#39;, &#39;pad_token&#39;: &#39;[PAD]&#39;, &#39;cls_token&#39;: &#39;[CLS]&#39;, &#39;mask_token&#39;: &#39;[MASK]&#39;}, clean_up_tokenization_spaces=True, added_tokens_decoder={
	0: AddedToken(&quot;[PAD]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken(&quot;[UNK]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken(&quot;[CLS]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	3: AddedToken(&quot;[SEP]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	4: AddedToken(&quot;[MASK]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
), mlm=True, mlm_probability=0.15, mask_replace_prob=0.8, random_replace_prob=0.1, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors=&#39;pt&#39;, seed=None)</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000,&quot;referenced_widgets&quot;:[&quot;9b8cee7282f0494ab085d9d723d298f2&quot;,&quot;8c424edf498343d895240c8bd0a0237a&quot;,&quot;deb55c56b79942b8acd2d89bde1fb0ad&quot;,&quot;b2c403c5af9044a6b39ce4d8da1cb880&quot;,&quot;09459fb6607047c699d63edfd149f4dd&quot;,&quot;153673333bc441cd86f25dd7afd647e5&quot;,&quot;aa808652a6d94f5596a4a26ac3748b7b&quot;,&quot;a5271ec605da4fe6adfb9e88dc15add3&quot;,&quot;8df41715f7364132937dd39c359e111e&quot;,&quot;c59cf3686d87419797252166b41be816&quot;,&quot;95a0e2f63588488e97e52b8e40abac17&quot;,&quot;d7e8408a68ef4a84ba2a902cba1446d1&quot;,&quot;d2d661321dd743eea752da717e1d79d9&quot;,&quot;c13ee0c01aba4a55983ab1b8aee9b77b&quot;,&quot;90e8c8dc820a44edb4d2171397554e33&quot;,&quot;e1a1ff0903f443ddbc73e6e45de141b9&quot;,&quot;023c4ee07cf542728c83a8bd33e65631&quot;,&quot;3bf044d5afcc4069b130cfce2ef6d346&quot;,&quot;3248741813db43968114eb7fa204facc&quot;,&quot;13637e68eaf7479cb5a0f277f9022f19&quot;,&quot;bd66cbf672c045d0a1bb5ab9e3472888&quot;,&quot;1e84d0c541ad40bcae10ffbe61d84ad4&quot;]}"
data-collapsed="true" id="UxxfABINaLae"
data-outputId="93755891-d5ce-4e35-d961-028dd278a6dd">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === Load pre-trained model ===</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForMaskedLM.from_pretrained(<span class="st">&quot;Rostlab/prot_bert&quot;</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === Training Arguments ===</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">&quot;./results&quot;</span>,</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">&quot;epoch&quot;</span>,</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">&quot;no&quot;</span>,</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    logging_dir<span class="op">=</span><span class="st">&quot;./logs&quot;</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co"># === Trainer ===</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized_dataset_train,</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>tokenized_dataset_test,</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a><span class="co"># === Train ===</span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a><span class="co"># === Final evaluation ===</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>eval_results <span class="op">=</span> trainer.evaluate()</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">✅ Final eval loss: </span><span class="sc">{</span>eval_results[<span class="st">&#39;eval_loss&#39;</span>]<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb26"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;9b8cee7282f0494ab085d9d723d298f2&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb27"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;d7e8408a68ef4a84ba2a902cba1446d1&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stderr">
<pre><code>Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForMaskedLM: [&#39;bert.pooler.dense.bias&#39;, &#39;bert.pooler.dense.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.seq_relationship.weight&#39;]
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/tmp/ipython-input-26-1991168294.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
</code></pre>
</div>
<div class="output display_data">
<pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre>
</div>
<div class="output stream stderr">
<pre><code>wandb: Logging into wandb.ai. (Learn how to deploy a W&amp;B server locally: https://wandb.me/wandb-server)
wandb: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models
wandb: Paste an API key from your profile and hit enter:</code></pre>
</div>
<div class="output stream stdout">
<pre><code> ··········
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>wandb: WARNING If you&#39;re specifying your api key in code, ensure this code is not shared publicly.
wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.
wandb: No netrc file found, creating one.
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: Currently logged in as: amartyahatua to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
</code></pre>
</div>
<div class="output display_data">
Tracking run with wandb version 0.21.0
</div>
<div class="output display_data">
Run data is saved locally in <code>/content/wandb/run-20250713_032047-9jn40p96</code>
</div>
<div class="output display_data">
Syncing run <strong><a href='https://wandb.ai/amartyahatua/huggingface/runs/9jn40p96' target="_blank">./results</a></strong> to <a href='https://wandb.ai/amartyahatua/huggingface' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target="_blank">docs</a>)<br>
</div>
<div class="output display_data">
 View project at <a href='https://wandb.ai/amartyahatua/huggingface' target="_blank">https://wandb.ai/amartyahatua/huggingface</a>
</div>
<div class="output display_data">
 View run at <a href='https://wandb.ai/amartyahatua/huggingface/runs/9jn40p96' target="_blank">https://wandb.ai/amartyahatua/huggingface/runs/9jn40p96</a>
</div>
<div class="output display_data">

    <div>
      
      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [3750/3750 22:42, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>20</td>
      <td>1.600300</td>
    </tr>
    <tr>
      <td>40</td>
      <td>1.314400</td>
    </tr>
    <tr>
      <td>60</td>
      <td>1.295200</td>
    </tr>
    <tr>
      <td>80</td>
      <td>1.275700</td>
    </tr>
    <tr>
      <td>100</td>
      <td>1.304400</td>
    </tr>
    <tr>
      <td>120</td>
      <td>1.281600</td>
    </tr>
    <tr>
      <td>140</td>
      <td>1.273500</td>
    </tr>
    <tr>
      <td>160</td>
      <td>1.260100</td>
    </tr>
    <tr>
      <td>180</td>
      <td>1.275300</td>
    </tr>
    <tr>
      <td>200</td>
      <td>1.261900</td>
    </tr>
    <tr>
      <td>220</td>
      <td>1.272600</td>
    </tr>
    <tr>
      <td>240</td>
      <td>1.267700</td>
    </tr>
    <tr>
      <td>260</td>
      <td>1.255200</td>
    </tr>
    <tr>
      <td>280</td>
      <td>1.264400</td>
    </tr>
    <tr>
      <td>300</td>
      <td>1.239000</td>
    </tr>
    <tr>
      <td>320</td>
      <td>1.263500</td>
    </tr>
    <tr>
      <td>340</td>
      <td>1.247700</td>
    </tr>
    <tr>
      <td>360</td>
      <td>1.251200</td>
    </tr>
    <tr>
      <td>380</td>
      <td>1.241400</td>
    </tr>
    <tr>
      <td>400</td>
      <td>1.258700</td>
    </tr>
    <tr>
      <td>420</td>
      <td>1.265500</td>
    </tr>
    <tr>
      <td>440</td>
      <td>1.259500</td>
    </tr>
    <tr>
      <td>460</td>
      <td>1.247900</td>
    </tr>
    <tr>
      <td>480</td>
      <td>1.249500</td>
    </tr>
    <tr>
      <td>500</td>
      <td>1.236400</td>
    </tr>
    <tr>
      <td>520</td>
      <td>1.237700</td>
    </tr>
    <tr>
      <td>540</td>
      <td>1.258500</td>
    </tr>
    <tr>
      <td>560</td>
      <td>1.233700</td>
    </tr>
    <tr>
      <td>580</td>
      <td>1.245900</td>
    </tr>
    <tr>
      <td>600</td>
      <td>1.237800</td>
    </tr>
    <tr>
      <td>620</td>
      <td>1.240800</td>
    </tr>
    <tr>
      <td>640</td>
      <td>1.247600</td>
    </tr>
    <tr>
      <td>660</td>
      <td>1.254100</td>
    </tr>
    <tr>
      <td>680</td>
      <td>1.234200</td>
    </tr>
    <tr>
      <td>700</td>
      <td>1.243500</td>
    </tr>
    <tr>
      <td>720</td>
      <td>1.250300</td>
    </tr>
    <tr>
      <td>740</td>
      <td>1.236800</td>
    </tr>
    <tr>
      <td>760</td>
      <td>1.232900</td>
    </tr>
    <tr>
      <td>780</td>
      <td>1.245300</td>
    </tr>
    <tr>
      <td>800</td>
      <td>1.230700</td>
    </tr>
    <tr>
      <td>820</td>
      <td>1.229500</td>
    </tr>
    <tr>
      <td>840</td>
      <td>1.234600</td>
    </tr>
    <tr>
      <td>860</td>
      <td>1.245700</td>
    </tr>
    <tr>
      <td>880</td>
      <td>1.225600</td>
    </tr>
    <tr>
      <td>900</td>
      <td>1.231300</td>
    </tr>
    <tr>
      <td>920</td>
      <td>1.221400</td>
    </tr>
    <tr>
      <td>940</td>
      <td>1.229600</td>
    </tr>
    <tr>
      <td>960</td>
      <td>1.230000</td>
    </tr>
    <tr>
      <td>980</td>
      <td>1.231100</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>1.222200</td>
    </tr>
    <tr>
      <td>1020</td>
      <td>1.220200</td>
    </tr>
    <tr>
      <td>1040</td>
      <td>1.227800</td>
    </tr>
    <tr>
      <td>1060</td>
      <td>1.215900</td>
    </tr>
    <tr>
      <td>1080</td>
      <td>1.224500</td>
    </tr>
    <tr>
      <td>1100</td>
      <td>1.235900</td>
    </tr>
    <tr>
      <td>1120</td>
      <td>1.226500</td>
    </tr>
    <tr>
      <td>1140</td>
      <td>1.207400</td>
    </tr>
    <tr>
      <td>1160</td>
      <td>1.217600</td>
    </tr>
    <tr>
      <td>1180</td>
      <td>1.210600</td>
    </tr>
    <tr>
      <td>1200</td>
      <td>1.211600</td>
    </tr>
    <tr>
      <td>1220</td>
      <td>1.222400</td>
    </tr>
    <tr>
      <td>1240</td>
      <td>1.208100</td>
    </tr>
    <tr>
      <td>1260</td>
      <td>1.218800</td>
    </tr>
    <tr>
      <td>1280</td>
      <td>1.215700</td>
    </tr>
    <tr>
      <td>1300</td>
      <td>1.207000</td>
    </tr>
    <tr>
      <td>1320</td>
      <td>1.187400</td>
    </tr>
    <tr>
      <td>1340</td>
      <td>1.200000</td>
    </tr>
    <tr>
      <td>1360</td>
      <td>1.215400</td>
    </tr>
    <tr>
      <td>1380</td>
      <td>1.219800</td>
    </tr>
    <tr>
      <td>1400</td>
      <td>1.185700</td>
    </tr>
    <tr>
      <td>1420</td>
      <td>1.211300</td>
    </tr>
    <tr>
      <td>1440</td>
      <td>1.191400</td>
    </tr>
    <tr>
      <td>1460</td>
      <td>1.208200</td>
    </tr>
    <tr>
      <td>1480</td>
      <td>1.200400</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>1.202900</td>
    </tr>
    <tr>
      <td>1520</td>
      <td>1.200000</td>
    </tr>
    <tr>
      <td>1540</td>
      <td>1.196500</td>
    </tr>
    <tr>
      <td>1560</td>
      <td>1.199100</td>
    </tr>
    <tr>
      <td>1580</td>
      <td>1.182000</td>
    </tr>
    <tr>
      <td>1600</td>
      <td>1.199700</td>
    </tr>
    <tr>
      <td>1620</td>
      <td>1.191400</td>
    </tr>
    <tr>
      <td>1640</td>
      <td>1.198700</td>
    </tr>
    <tr>
      <td>1660</td>
      <td>1.195900</td>
    </tr>
    <tr>
      <td>1680</td>
      <td>1.186900</td>
    </tr>
    <tr>
      <td>1700</td>
      <td>1.180400</td>
    </tr>
    <tr>
      <td>1720</td>
      <td>1.181700</td>
    </tr>
    <tr>
      <td>1740</td>
      <td>1.176200</td>
    </tr>
    <tr>
      <td>1760</td>
      <td>1.173600</td>
    </tr>
    <tr>
      <td>1780</td>
      <td>1.164600</td>
    </tr>
    <tr>
      <td>1800</td>
      <td>1.192300</td>
    </tr>
    <tr>
      <td>1820</td>
      <td>1.194300</td>
    </tr>
    <tr>
      <td>1840</td>
      <td>1.191300</td>
    </tr>
    <tr>
      <td>1860</td>
      <td>1.166700</td>
    </tr>
    <tr>
      <td>1880</td>
      <td>1.187700</td>
    </tr>
    <tr>
      <td>1900</td>
      <td>1.185200</td>
    </tr>
    <tr>
      <td>1920</td>
      <td>1.180700</td>
    </tr>
    <tr>
      <td>1940</td>
      <td>1.170100</td>
    </tr>
    <tr>
      <td>1960</td>
      <td>1.187200</td>
    </tr>
    <tr>
      <td>1980</td>
      <td>1.175900</td>
    </tr>
    <tr>
      <td>2000</td>
      <td>1.165700</td>
    </tr>
    <tr>
      <td>2020</td>
      <td>1.175800</td>
    </tr>
    <tr>
      <td>2040</td>
      <td>1.176600</td>
    </tr>
    <tr>
      <td>2060</td>
      <td>1.158300</td>
    </tr>
    <tr>
      <td>2080</td>
      <td>1.146800</td>
    </tr>
    <tr>
      <td>2100</td>
      <td>1.174800</td>
    </tr>
    <tr>
      <td>2120</td>
      <td>1.181200</td>
    </tr>
    <tr>
      <td>2140</td>
      <td>1.178700</td>
    </tr>
    <tr>
      <td>2160</td>
      <td>1.178100</td>
    </tr>
    <tr>
      <td>2180</td>
      <td>1.166800</td>
    </tr>
    <tr>
      <td>2200</td>
      <td>1.177400</td>
    </tr>
    <tr>
      <td>2220</td>
      <td>1.162500</td>
    </tr>
    <tr>
      <td>2240</td>
      <td>1.151100</td>
    </tr>
    <tr>
      <td>2260</td>
      <td>1.182700</td>
    </tr>
    <tr>
      <td>2280</td>
      <td>1.179500</td>
    </tr>
    <tr>
      <td>2300</td>
      <td>1.147400</td>
    </tr>
    <tr>
      <td>2320</td>
      <td>1.149100</td>
    </tr>
    <tr>
      <td>2340</td>
      <td>1.186100</td>
    </tr>
    <tr>
      <td>2360</td>
      <td>1.155800</td>
    </tr>
    <tr>
      <td>2380</td>
      <td>1.175900</td>
    </tr>
    <tr>
      <td>2400</td>
      <td>1.165400</td>
    </tr>
    <tr>
      <td>2420</td>
      <td>1.134200</td>
    </tr>
    <tr>
      <td>2440</td>
      <td>1.175000</td>
    </tr>
    <tr>
      <td>2460</td>
      <td>1.154100</td>
    </tr>
    <tr>
      <td>2480</td>
      <td>1.160900</td>
    </tr>
    <tr>
      <td>2500</td>
      <td>1.180000</td>
    </tr>
    <tr>
      <td>2520</td>
      <td>1.160700</td>
    </tr>
    <tr>
      <td>2540</td>
      <td>1.143400</td>
    </tr>
    <tr>
      <td>2560</td>
      <td>1.165900</td>
    </tr>
    <tr>
      <td>2580</td>
      <td>1.145000</td>
    </tr>
    <tr>
      <td>2600</td>
      <td>1.159500</td>
    </tr>
    <tr>
      <td>2620</td>
      <td>1.164500</td>
    </tr>
    <tr>
      <td>2640</td>
      <td>1.171600</td>
    </tr>
    <tr>
      <td>2660</td>
      <td>1.166400</td>
    </tr>
    <tr>
      <td>2680</td>
      <td>1.164900</td>
    </tr>
    <tr>
      <td>2700</td>
      <td>1.144600</td>
    </tr>
    <tr>
      <td>2720</td>
      <td>1.137100</td>
    </tr>
    <tr>
      <td>2740</td>
      <td>1.159900</td>
    </tr>
    <tr>
      <td>2760</td>
      <td>1.148000</td>
    </tr>
    <tr>
      <td>2780</td>
      <td>1.142900</td>
    </tr>
    <tr>
      <td>2800</td>
      <td>1.141600</td>
    </tr>
    <tr>
      <td>2820</td>
      <td>1.160400</td>
    </tr>
    <tr>
      <td>2840</td>
      <td>1.137300</td>
    </tr>
    <tr>
      <td>2860</td>
      <td>1.138700</td>
    </tr>
    <tr>
      <td>2880</td>
      <td>1.149600</td>
    </tr>
    <tr>
      <td>2900</td>
      <td>1.157900</td>
    </tr>
    <tr>
      <td>2920</td>
      <td>1.142400</td>
    </tr>
    <tr>
      <td>2940</td>
      <td>1.125900</td>
    </tr>
    <tr>
      <td>2960</td>
      <td>1.139800</td>
    </tr>
    <tr>
      <td>2980</td>
      <td>1.132700</td>
    </tr>
    <tr>
      <td>3000</td>
      <td>1.139000</td>
    </tr>
    <tr>
      <td>3020</td>
      <td>1.131700</td>
    </tr>
    <tr>
      <td>3040</td>
      <td>1.149000</td>
    </tr>
    <tr>
      <td>3060</td>
      <td>1.131900</td>
    </tr>
    <tr>
      <td>3080</td>
      <td>1.153100</td>
    </tr>
    <tr>
      <td>3100</td>
      <td>1.143500</td>
    </tr>
    <tr>
      <td>3120</td>
      <td>1.152000</td>
    </tr>
    <tr>
      <td>3140</td>
      <td>1.164400</td>
    </tr>
    <tr>
      <td>3160</td>
      <td>1.128400</td>
    </tr>
    <tr>
      <td>3180</td>
      <td>1.131200</td>
    </tr>
    <tr>
      <td>3200</td>
      <td>1.125200</td>
    </tr>
    <tr>
      <td>3220</td>
      <td>1.127700</td>
    </tr>
    <tr>
      <td>3240</td>
      <td>1.137200</td>
    </tr>
    <tr>
      <td>3260</td>
      <td>1.146400</td>
    </tr>
    <tr>
      <td>3280</td>
      <td>1.138600</td>
    </tr>
    <tr>
      <td>3300</td>
      <td>1.141300</td>
    </tr>
    <tr>
      <td>3320</td>
      <td>1.137400</td>
    </tr>
    <tr>
      <td>3340</td>
      <td>1.134400</td>
    </tr>
    <tr>
      <td>3360</td>
      <td>1.132300</td>
    </tr>
    <tr>
      <td>3380</td>
      <td>1.160900</td>
    </tr>
    <tr>
      <td>3400</td>
      <td>1.139600</td>
    </tr>
    <tr>
      <td>3420</td>
      <td>1.143100</td>
    </tr>
    <tr>
      <td>3440</td>
      <td>1.129100</td>
    </tr>
    <tr>
      <td>3460</td>
      <td>1.117100</td>
    </tr>
    <tr>
      <td>3480</td>
      <td>1.133300</td>
    </tr>
    <tr>
      <td>3500</td>
      <td>1.122900</td>
    </tr>
    <tr>
      <td>3520</td>
      <td>1.160100</td>
    </tr>
    <tr>
      <td>3540</td>
      <td>1.119400</td>
    </tr>
    <tr>
      <td>3560</td>
      <td>1.132300</td>
    </tr>
    <tr>
      <td>3580</td>
      <td>1.138800</td>
    </tr>
    <tr>
      <td>3600</td>
      <td>1.135900</td>
    </tr>
    <tr>
      <td>3620</td>
      <td>1.133500</td>
    </tr>
    <tr>
      <td>3640</td>
      <td>1.128000</td>
    </tr>
    <tr>
      <td>3660</td>
      <td>1.110700</td>
    </tr>
    <tr>
      <td>3680</td>
      <td>1.148900</td>
    </tr>
    <tr>
      <td>3700</td>
      <td>1.121600</td>
    </tr>
    <tr>
      <td>3720</td>
      <td>1.104400</td>
    </tr>
    <tr>
      <td>3740</td>
      <td>1.131900</td>
    </tr>
  </tbody>
</table><p>
</div>
<div class="output display_data">

    <div>
      
      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [250/250 00:53]
    </div>
    
</div>
<div class="output stream stdout">
<pre><code>
✅ Final eval loss: 1.1326
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="kvUJfO3u2fkd" data-outputId="6c8a5865-c43d-48e8-b8d2-d6ed7b85ed78">
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>trainer.save_model(<span class="st">&quot;/content/drive/MyDrive/PLM/finetuned_protein_roberta_SwissprotDatasets_BalancedSwissprot&quot;</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>tokenizer.save_pretrained(<span class="st">&quot;/content/drive/MyDrive/PLM/finetuned_protein_roberta_SwissprotDatasets_BalancedSwissprot&quot;</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="34">
<pre><code>(&#39;/content/drive/MyDrive/PLM/finetuned_protein_roberta_SwissprotDatasets_BalancedSwissprot/tokenizer_config.json&#39;,
 &#39;/content/drive/MyDrive/PLM/finetuned_protein_roberta_SwissprotDatasets_BalancedSwissprot/special_tokens_map.json&#39;,
 &#39;/content/drive/MyDrive/PLM/finetuned_protein_roberta_SwissprotDatasets_BalancedSwissprot/vocab.txt&#39;,
 &#39;/content/drive/MyDrive/PLM/finetuned_protein_roberta_SwissprotDatasets_BalancedSwissprot/added_tokens.json&#39;,
 &#39;/content/drive/MyDrive/PLM/finetuned_protein_roberta_SwissprotDatasets_BalancedSwissprot/tokenizer.json&#39;)</code></pre>
</div>
</div>
<section id="evaluation" class="cell markdown" id="hBe6EGiomFjT">
<h3>Evaluation</h3>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="zSuO0sQCkjN0" data-outputId="ac54b013-4442-4691-f674-71bdc4fc5d57">
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === Evaluate on test set ===</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>fill_mask <span class="op">=</span> pipeline(<span class="st">&quot;fill-mask&quot;</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>total <span class="op">=</span> <span class="dv">0</span></span></code></pre></div>
<div class="output stream stderr">
<pre><code>Device set to use cuda:0
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="Zc2oXO7-mNoa" data-outputId="ac457d56-082b-49aa-97dd-bc402d4b692d">
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>top1_correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>top5_correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> seq <span class="kw">in</span> test_df[<span class="st">&#39;Sequence&#39;</span>][:<span class="dv">500</span>]:</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(seq) <span class="op">&lt;</span> <span class="dv">5</span>:</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> torch.randint(<span class="dv">1</span>, <span class="bu">len</span>(seq)<span class="op">-</span><span class="dv">1</span>, (<span class="dv">1</span>,)).item()</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>    true_token <span class="op">=</span> seq[pos]</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    seq_masked <span class="op">=</span> <span class="bu">list</span>(seq)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    seq_masked[pos] <span class="op">=</span> tokenizer.mask_token</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    masked_input <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(seq_masked)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    masked_input <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>tokenizer<span class="sc">.</span>cls_token<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>masked_input<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>tokenizer<span class="sc">.</span>sep_token<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> fill_mask(masked_input)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>        top_preds <span class="op">=</span> [p[<span class="st">&#39;token_str&#39;</span>].strip() <span class="cf">for</span> p <span class="kw">in</span> preds]</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> true_token <span class="op">==</span> top_preds[<span class="dv">0</span>]:</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>            top1_correct <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> true_token <span class="kw">in</span> top_preds:</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>            top5_correct <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span>:</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Top-1 Accuracy: </span><span class="sc">{</span>top1_correct <span class="op">/</span> total<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Top-5 Accuracy: </span><span class="sc">{</span>top5_correct <span class="op">/</span> total<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Top-1 Accuracy: 0.3140
Top-5 Accuracy: 1.0000
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:105}"
id="q73FbM7NnhSD" data-outputId="3ee004e9-1009-4f9d-a9fc-1aa862c864d7">
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>masked_input</span></code></pre></div>
<div class="output execute_result" data-execution_count="33">
<div class="sourceCode" id="cb42"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;type&quot;</span><span class="fu">:</span><span class="st">&quot;string&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<script defer src="https://static.cloudflareinsights.com/beacon.min.js/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon='{"rayId":"960612e26c7e7274","version":"2025.7.0","r":1,"token":"0357a45f23a943f08700f7f9af191ee6","serverTiming":{"name":{"cfExtPri":true,"cfEdge":true,"cfOrigin":true,"cfL4":true,"cfSpeedBrain":true,"cfCacheStatus":true}}}' crossorigin="anonymous"></script>
</body>
</html>
